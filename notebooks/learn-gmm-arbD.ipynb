{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55eb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from typing import Tuple, Any\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import interflow as itf\n",
    "import interflow.prior as prior\n",
    "import interflow.fabrics\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "import interflow.gmm as gmm\n",
    "\n",
    "from torch import autograd\n",
    "from functorch import jacfwd, vmap\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA available, setting default tensor residence to GPU.')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('No CUDA device found!')\n",
    "print(itf.util.get_torch_device())\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e59ba-9c82-4f40-8e79-afc3f91a8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid']  = True\n",
    "mpl.rcParams['axes.grid.which']  = 'both'\n",
    "mpl.rcParams['xtick.minor.visible']  = True\n",
    "mpl.rcParams['ytick.minor.visible']  = True\n",
    "mpl.rcParams['xtick.minor.visible']  = True\n",
    "mpl.rcParams['axes.facecolor'] = 'white'\n",
    "mpl.rcParams['grid.color'] = '0.8'\n",
    "mpl.rcParams['grid.alpha'] = '0.5'\n",
    "mpl.rcParams['figure.figsize'] = (8, 4)\n",
    "mpl.rcParams['figure.titlesize'] = 12.5\n",
    "mpl.rcParams['font.size'] = 12.5\n",
    "mpl.rcParams['legend.fontsize'] = 12.5\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['text.usetex'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f48119",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    \"\"\"Take a tensor off the gpu and convert it to a numpy array on the CPU.\"\"\"\n",
    "    return var.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_likelihoods(\n",
    "    v: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_likelihood: int,\n",
    "    eps: int,\n",
    "    bs: int\n",
    ") -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"Draw samples from the probability flow and SDE models, and compute likelihoods.\"\"\"\n",
    "    sde_flow = stochastic_interpolant.SDEIntegrator(\n",
    "        v=v, s=s, dt=torch.tensor(1e-2), eps=eps, interpolant=interpolant, n_save=n_save, n_likelihood=n_likelihood\n",
    "    )\n",
    "\n",
    "    pflow = stochastic_interpolant.PFlowIntegrator(v=v, s=s,  \n",
    "                                                  method='dopri5', \n",
    "                                                  interpolant=interpolant,\n",
    "                                                  n_step=3)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x0_tests  = base(bs)\n",
    "        xfs_sde   = sde_flow.rollout_forward(x0_tests) # [n_save x bs x dim]\n",
    "        xf_sde    = grab(xfs_sde[-1].squeeze())        # [bs x dim]\n",
    "        \n",
    "        # ([n_likelihood, bs, dim], [bs])\n",
    "        x0s_sdeflow, dlogps_sdeflow = sde_flow.rollout_likelihood(xfs_sde[-1])\n",
    "        log_p0s = torch.reshape(\n",
    "            base.log_prob(x0s_sdeflow.reshape((n_likelihood*bs, ndim))),\n",
    "            (n_likelihood, bs)\n",
    "        )\n",
    "        logpx_sdeflow = torch.mean(log_p0s, axis=0) - dlogps_sdeflow\n",
    "\n",
    "\n",
    "    logp0                  = base.log_prob(x0_tests)            # [bs]\n",
    "    xfs_pflow, dlogp_pflow = pflow.rollout(x0_tests)            # [n_save x bs x dim], [n_save x bs]\n",
    "    logpx_pflow            = logp0 + dlogp_pflow[-1].squeeze()  # [bs]\n",
    "    xf_pflow               = grab(xfs_pflow[-1].squeeze())      # [bs x dim]\n",
    "\n",
    "\n",
    "    return xf_sde, logpx_sdeflow, xf_pflow, logpx_pflow\n",
    "\n",
    "\n",
    "def log_metrics(\n",
    "    v: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    exact_interpolant: gmm.GMMInterpolant,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_likelihood: int,\n",
    "    likelihood_bs: int, \n",
    "    v_loss: torch.tensor,\n",
    "    s_loss: torch.tensor,\n",
    "    loss: torch.tensor,\n",
    "    v_grad: torch.tensor,\n",
    "    s_grad: torch.tensor,\n",
    "    eps: torch.tensor,\n",
    "    data_dict: dict\n",
    ") -> None:\n",
    "    # log loss and gradient data\n",
    "    v_loss   = grab(v_loss).mean();   data_dict['v_losses'].append(v_loss)\n",
    "    s_loss   = grab(s_loss).mean();   data_dict['s_losses'].append(s_loss)\n",
    "    loss     = grab(loss).mean();     data_dict['losses'].append(loss)\n",
    "    v_grad   = grab(v_grad).mean();   data_dict['v_grads'].append(v_grad)\n",
    "    s_grad   = grab(s_grad).mean();   data_dict['s_grads'].append(s_grad)\n",
    "\n",
    "    \n",
    "    # compute and log likelihood data\n",
    "    _, logpx_sdeflow, _, logpx_pflow = compute_likelihoods(\n",
    "        v, s, interpolant, n_save, n_likelihood, eps, likelihood_bs)\n",
    "    \n",
    "    \n",
    "    # compute kl and log data\n",
    "    kl_pflow, kl_sdeflow = compute_kl(v, s, exact_interpolant, interpolant, eps, likelihood_bs)\n",
    "    kl_pflow = grab(kl_pflow).mean(); data_dict['kl_pflow'].append(kl_pflow)\n",
    "    kl_sdeflow = grab(kl_sdeflow).mean(); data_dict['kl_sdeflow'].append(kl_sdeflow)\n",
    "    \n",
    "    logpx_sdeflow = grab(logpx_sdeflow).mean(); data_dict['logps_sdeflow'].append(logpx_sdeflow)\n",
    "    logpx_pflow = grab(logpx_pflow).mean(); data_dict['logps_pflow'].append(logpx_pflow)\n",
    "    \n",
    "    \n",
    "def make_plots(\n",
    "    v: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_likelihood: int,\n",
    "    likelihood_bs: int,\n",
    "    counter: int,\n",
    "    metrics_freq: int,\n",
    "    eps: torch.tensor,\n",
    "    data_dict: dict\n",
    ") -> None:\n",
    "    \"\"\"Make plots to visualize samples and evolution of the likelihood.\"\"\"\n",
    "    # compute likelihood and samples for SDE and probability flow.\n",
    "    xf_sde, logpx_sdeflow, xf_pflow, logpx_pflow = compute_likelihoods(\n",
    "        v, s, interpolant, n_save, n_likelihood, eps, likelihood_bs\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ### plot the loss, test logp, and samples from interpolant flow\n",
    "    fig, axes = plt.subplots(1,4, figsize=(16,4))\n",
    "    print(\"EPOCH:\", counter)\n",
    "    print(\"LOSS, GRAD:\", loss, v_grad, s_grad)\n",
    "\n",
    "\n",
    "    # plot loss over time.\n",
    "    nsaves = len(data_dict['losses'])\n",
    "    epochs = np.arange(nsaves)*metrics_freq\n",
    "    axes[0].plot(epochs, data_dict['losses'], label=\" v + s\")\n",
    "    axes[0].plot(epochs, data_dict['v_losses'], label=\"v\")\n",
    "    axes[0].plot(epochs, data_dict['s_losses'], label = \"s\" )\n",
    "    axes[0].set_title(\"LOSS\")\n",
    "    axes[0].legend()\n",
    "\n",
    "\n",
    "    # plot samples from SDE.\n",
    "    axes[1].scatter(\n",
    "        xf_sde[:,0], xf_sde[:,1], vmin=0.0, vmax=0.05, alpha = 0.2, c=grab(torch.exp(logpx_sdeflow).detach()))\n",
    "    axes[1].set_xlim(-10,10)\n",
    "    axes[1].set_ylim(-10,10)\n",
    "    axes[1].set_title(\"Dims 0,1 of Samples from SDE\", fontsize=14)\n",
    "\n",
    "\n",
    "    # plot samples from pflow\n",
    "    axes[2].scatter(\n",
    "        xf_pflow[:,0], xf_pflow[:,1], vmin=0.0, vmax=0.05, alpha = 0.2, c=grab(torch.exp(logpx_pflow).detach()))\n",
    "    axes[2].set_xlim(-10,10)\n",
    "    axes[2].set_ylim(-10,10)\n",
    "    axes[2].set_title(\"Dims 0,1 of Samples from PFlow\", fontsize=14)\n",
    "\n",
    "\n",
    "    # plot likelihood estimates.\n",
    "    print( data_dict['kl_pflow'])\n",
    "    axes[3].plot(epochs, data_dict['kl_pflow'],   label='pflow', color='purple')\n",
    "    axes[3].plot(epochs, data_dict['kl_sdeflow'], label='sde',   color='red')\n",
    "    axes[3].set_title(r\"$KL(\\rho_1(x) | \\hat\\rho(1,x)$\")\n",
    "    axes[3].legend(loc='best')\n",
    "    ymax = max(data_dict['kl_pflow'])\n",
    "    axes[3].set_ylim(-5,ymax + ymax*.01)\n",
    "\n",
    "\n",
    "    fig.suptitle(r\"$\\epsilon = $\" + str(grab(eps)) + r\" $n_{likelihood} = $\" + str(n_likelihood), fontsize=16, y = 1.05)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train_step(\n",
    "    prior_bs: int,\n",
    "    target_bs: int,\n",
    "    N_t: int,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    opt: Any,\n",
    "    sched: Any\n",
    "):\n",
    "    \"\"\"\n",
    "    Take a single step of optimization on the training set.\n",
    "    \"\"\"\n",
    "    opt.zero_grad()\n",
    "\n",
    "\n",
    "    # construct batch\n",
    "    x0s = base(prior_bs)\n",
    "    x1s = target(target_bs)\n",
    "    ts  = torch.rand(size=(N_t,))\n",
    "\n",
    "\n",
    "    # compute the loss\n",
    "    loss_start = time.perf_counter()\n",
    "    loss_val, (loss_v, loss_s) = stochastic_interpolant.loss_sv(\n",
    "        v, s, x0s, x1s, ts, interpolant, loss_fac=loss_fac\n",
    "    )\n",
    "    loss_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    # compute the gradient\n",
    "    backprop_start = time.perf_counter()\n",
    "    loss_val.backward()\n",
    "    v_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(v.parameters(), float('inf'))])\n",
    "    s_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(s.parameters(), float('inf'))])\n",
    "    backprop_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    # perform the update.\n",
    "    update_start = time.perf_counter()\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "    update_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    if counter < 5:\n",
    "        print(f'[Loss: {loss_end - loss_start}], [Backprop: {backprop_end-backprop_start}], [Update: {update_end-update_start}].')\n",
    "\n",
    "\n",
    "    return loss_val.detach(), loss_v.detach(), loss_s.detach(), v_grad.detach(), s_grad.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2670e713-e83c-4ec7-84f3-3dd4f0912f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(v,s, exact_interpolant, interpolant, eps = torch.tensor(2.0), bs = 500, n_likelihood=5):\n",
    "    \"\"\"\n",
    "    Currently for ODE only\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    sde_flow = stochastic_interpolant.SDEIntegrator(\n",
    "        v=v, s=s, dt=torch.tensor(1e-2), eps=eps, interpolant=interpolant, n_save=5, n_likelihood=n_likelihood\n",
    "    )\n",
    "\n",
    "    pflow = stochastic_interpolant.PFlowIntegrator(v=v, s=s,  \n",
    "                                                  method='dopri5', \n",
    "                                                  interpolant=interpolant,\n",
    "                                                  n_step=3)\n",
    "    \n",
    "    x1s = exact_interpolant.sample_rho1(bs)\n",
    "    log_rho1 = exact_interpolant.log_rho1(x1s)\n",
    "    \n",
    "    x0s_pflow, dlogp_pflow = pflow.rollout(x1s, reverse=True)   # [n_save x bs x dim], [n_save x bs]\n",
    "    x0_pflow               = grab(x0s_pflow[-1].squeeze())      # [bs x dim]\n",
    "    logp0                  = base.log_prob(x0s_pflow[-1])       # [bs]\n",
    "    log_rho1_hat_ode            = logp0 - dlogp_pflow[-1].squeeze()  # [bs]\n",
    "    \n",
    "    \n",
    "    # ([n_likelihood, bs, dim], [bs])\n",
    "    with torch.no_grad():\n",
    "        x0s_sdeflow, dlogps_sdeflow = sde_flow.rollout_likelihood(x1s)\n",
    "        log_p0s = torch.reshape(\n",
    "            base.log_prob(x0s_sdeflow.reshape((n_likelihood*bs, ndim))),\n",
    "            (n_likelihood, bs)\n",
    "        )\n",
    "        log_rho1_hat_sde = torch.mean(log_p0s, axis=0) - dlogps_sdeflow\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (log_rho1 - log_rho1_hat_ode).mean(), (log_rho1 - log_rho1_hat_sde).mean()\n",
    "\n",
    "\n",
    "compute_kl(v,s,exact_interpolant,interpolant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e86c89-70d8-4de8-8034-2a2786a05db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_v_diff(v,s, exact_interpolant):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ce2eb",
   "metadata": {},
   "source": [
    "### Define target GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397336c3-cf0c-43d9-9473-0876c003c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_random_covs(N: int, d: int):\n",
    "    Cs = torch.zeros(N, d, d)\n",
    "    for ii in range(N):\n",
    "        C = torch.randn(d, d)\n",
    "        Cs[ii] = (C.T @ C + 0.5*torch.eye(d))/torch.sqrt(torch.tensor(d))\n",
    "    \n",
    "    return Cs\n",
    "\n",
    "\n",
    "def setup_random_means(N: int, d: int, scale: float):\n",
    "    return scale*torch.randn((N, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045a713-d8f6-424a-9f45-459e0708c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim        = 50\n",
    "N0          = 1\n",
    "N1          = 5\n",
    "scale       = 4\n",
    "gamma_type  = 'brownian'\n",
    "path        = 'linear'\n",
    "p0s         = (torch.ones(N0) / N0)\n",
    "p1s         = (torch.ones(N1) / N1)\n",
    "mu0s        = setup_random_means(N0, ndim, scale)\n",
    "mu1s        = setup_random_means(N1, ndim, scale)\n",
    "C0s         = torch.eye(ndim).unsqueeze(0)\n",
    "C1s         = setup_random_covs(N1, ndim)\n",
    "print(C0s.shape)\n",
    "\n",
    "exact_interpolant = gmm.GMMInterpolant(\n",
    "    p0s, p1s, mu0s, mu1s, C0s, C1s, path, gamma_type, device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "target = lambda bs: exact_interpolant.sample_rho1(bs)\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import gridspec\n",
    "unit_size = ndim / 1\n",
    "fig = plt.figure(figsize = (unit_size * 2,unit_size * 1), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ndim, ndim, figure=fig)\n",
    "gs.update(wspace=0.5)\n",
    "\n",
    "# ensemble = cfgs_all_batch[-1][::1]\n",
    "target_samples = grab(target(10000))\n",
    "# print(true.shape)\n",
    "for i in range(5):\n",
    "    for j in range(i):\n",
    "        ax = plt.subplot(gs[i, j], )\n",
    "        \n",
    "        # ax.scatter(ensemble[:,j], ensemble[:,i], alpha=0.02)\n",
    "        ax.scatter(target_samples[:,j],target_samples[:,i], alpha=0.02)\n",
    "        ax.set_xlim(-15,15)\n",
    "        ax.set_ylim(-15,15)\n",
    "        ax.set_xticks([])\n",
    "        # ax.set_yticks([])\n",
    "        \n",
    "bottom = 0.01; left=0.01\n",
    "top=1.-bottom; right = 1.-0.5\n",
    "wspace= 0.0  # set to zero for no spacing\n",
    "hspace= 0.2\n",
    "plt.subplots_adjust(top=top, bottom=bottom, left=left, right=right, \n",
    "                    wspace=wspace, hspace=hspace)\n",
    "fig.text(x = 0.01, y = 0.974, s= \"Cross sections of \" + str(ndim) + \"-dimensional target GMM\", fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a47a52",
   "metadata": {},
   "source": [
    "### Define Base Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27190b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMbase(itf.prior.Prior):\n",
    "    def __init__(self, exact_interpolant):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.exact_interpolant = exact_interpolant\n",
    "            \n",
    "    def log_prob(self, x):\n",
    "        return self.exact_interpolant.log_rho0(x)\n",
    "    \n",
    "    def forward(self, bs):\n",
    "        return self.exact_interpolant.sample_rho0(bs)\n",
    "    \n",
    "    \n",
    "base = GMMbase(exact_interpolant)\n",
    "base_samples = grab(base(10000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eccf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3,))\n",
    "plt.scatter(base_samples[:,0], base_samples[:,1],  label = 'base', alpha = 0.1);\n",
    "plt.scatter(target_samples[:,0], target_samples[:,1], alpha = 0.1);\n",
    "plt.title(\"Bimodal Target\")\n",
    "plt.title(\"Base vs Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4caad",
   "metadata": {},
   "source": [
    "### Define Interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb13153",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolant  = stochastic_interpolant.Interpolant(path=path, gamma_type=gamma_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9c81d",
   "metadata": {},
   "source": [
    "### Define velocity field and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba805f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr      = 2e-3\n",
    "hidden_sizes = [100, 100, 100, 100]\n",
    "in_size      = (ndim+1)\n",
    "out_size     = (ndim)\n",
    "inner_act    = 'relu'\n",
    "final_act    = 'none'\n",
    "print_model  = False\n",
    "\n",
    "\n",
    "v     = itf.fabrics.make_fc_net(hidden_sizes=hidden_sizes, in_size=in_size, out_size=out_size, inner_act=inner_act, final_act=final_act)\n",
    "s     = itf.fabrics.make_fc_net(hidden_sizes=hidden_sizes, in_size=in_size, out_size=out_size, inner_act=inner_act, final_act=final_act)\n",
    "opt   = torch.optim.Adam([*v.parameters(), *s.parameters()], lr=base_lr)\n",
    "sched = torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=1500, gamma=0.4)\n",
    "\n",
    "\n",
    "eps          = torch.tensor(0.5)\n",
    "N_era        = 14\n",
    "N_epoch      = 500\n",
    "N_t          = 50    # number of time steps in batch (e.g. to make samples from rho_t)\n",
    "plot_bs      = 2000 # number of samples to use when plotting\n",
    "prior_bs     = 25   # number of samples from rho_0 in batch\n",
    "target_bs    = 100   # number of samples from rho_1 in batch\n",
    "metrics_freq = 50   # how often to log metrics, e.g. if logp is not super cheap don't do it everytime\n",
    "plot_freq    = 500  # how often to plot\n",
    "n_save       = 10   # how often to checkpoint SDE integrator\n",
    "loss_fac     = 4.0 # ratio of learning rates for w to v\n",
    "n_likelihood = 20    # number of trajectories used to compute the SDE likelihood\n",
    "\n",
    "\n",
    "if print_model:\n",
    "    print(\"Here's the model v, s:\", v, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f33113",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'losses': [],\n",
    "    'v_losses': [],\n",
    "    's_losses': [],\n",
    "    'v_grads': [],\n",
    "    's_grads': [],\n",
    "    'times': [],\n",
    "    'logps_pflow': [],\n",
    "    'logps_sdeflow': [],\n",
    "    'kl_pflow': [],\n",
    "    'kl_sdeflow': []\n",
    "}\n",
    "\n",
    "counter = 1\n",
    "for i, era in enumerate(range(N_era)):\n",
    "    for j, epoch in enumerate(range(N_epoch)):\n",
    "        loss, v_loss, s_loss, v_grad, s_grad = train_step(\n",
    "            prior_bs, target_bs, N_t, interpolant, opt, sched\n",
    "        )\n",
    "\n",
    "\n",
    "        if (counter - 1) % metrics_freq == 0:\n",
    "            log_metrics(v, s, exact_interpolant, interpolant, n_save, n_likelihood, prior_bs, v_loss, \n",
    "                        s_loss, loss, v_grad, s_grad, eps, data_dict)\n",
    "\n",
    "\n",
    "        if (counter - 1) % plot_freq == 0:\n",
    "            make_plots(v, s, interpolant, n_save, n_likelihood, plot_bs, counter, metrics_freq, eps, data_dict)\n",
    "\n",
    "\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f81203-7a1e-4795-b5d8-24b02a72dbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afm",
   "language": "python",
   "name": "afm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
